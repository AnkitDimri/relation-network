{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NIPS Implementation Challenge](https://nurture.ai/nips-challenge/p/089570b9-fe63-43af-8a25-76117d2a1c21)\n",
    "\n",
    "Implementation of [A simple neural network module\n",
    "for relational reasoning](https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf) by [Vikram Voleti](https://voletiv.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer, Input, Reshape, Embedding, LSTM, Dense, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS OPTIONS\n",
    "K.set_image_data_format = \"channels_last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE NAMES\n",
    "# Scenes\n",
    "state_description_train_file = '/home/voletiv/Datasets/CLEVR_v1.0/scenes/CLEVR_train_scenes.json'\n",
    "state_description_val_file = '/home/voletiv/Datasets/CLEVR_v1.0/scenes/CLEVR_val_scenes.json'\n",
    "# Questions\n",
    "questions_train_file = '/home/voletiv/Datasets/CLEVR_v1.0/questions/CLEVR_train_questions.json'\n",
    "questions_val_file = '/home/voletiv/Datasets/CLEVR_v1.0/questions/CLEVR_val_questions.json'\n",
    "questions_test_file = '/home/voletiv/Datasets/CLEVR_v1.0/questions/CLEVR_test_questions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATION NETWORK PARAMS\n",
    "WORD_EMBEDDING_DIM = 32\n",
    "LSTM_UNITS = 256\n",
    "G_FC1, G_FC2, G_FC3, G_FC4 = 512, 512, 512, 512\n",
    "F_FC1, F_FC2, F_DROPOUT2, F_FC3 = 512, 1024, 0.02, 29\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "optimizer = Adam(lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Dealing with questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading questions from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .json files\n",
    "questions_train = json.load(open(questions_train_file))\n",
    "questions_val = json.load(open(questions_val_file))\n",
    "questions_test = json.load(open(questions_test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'yes',\n",
       " 'image_filename': 'CLEVR_train_000000.png',\n",
       " 'image_index': 0,\n",
       " 'program': [{'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
       "  {'function': 'filter_size', 'inputs': [0], 'value_inputs': ['large']},\n",
       "  {'function': 'filter_color', 'inputs': [1], 'value_inputs': ['green']},\n",
       "  {'function': 'count', 'inputs': [2], 'value_inputs': []},\n",
       "  {'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
       "  {'function': 'filter_size', 'inputs': [4], 'value_inputs': ['large']},\n",
       "  {'function': 'filter_color', 'inputs': [5], 'value_inputs': ['purple']},\n",
       "  {'function': 'filter_material', 'inputs': [6], 'value_inputs': ['metal']},\n",
       "  {'function': 'filter_shape', 'inputs': [7], 'value_inputs': ['cube']},\n",
       "  {'function': 'count', 'inputs': [8], 'value_inputs': []},\n",
       "  {'function': 'greater_than', 'inputs': [3, 9], 'value_inputs': []}],\n",
       " 'question': 'Are there more big green things than large purple shiny cubes?',\n",
       " 'question_family_index': 2,\n",
       " 'question_index': 0,\n",
       " 'split': 'train'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_train[\"questions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '2',\n",
       " 'image_filename': 'CLEVR_train_000000.png',\n",
       " 'image_index': 0,\n",
       " 'program': [{'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
       "  {'function': 'filter_size', 'inputs': [0], 'value_inputs': ['small']},\n",
       "  {'function': 'filter_color', 'inputs': [1], 'value_inputs': ['cyan']},\n",
       "  {'function': 'filter_material', 'inputs': [2], 'value_inputs': ['rubber']},\n",
       "  {'function': 'unique', 'inputs': [3], 'value_inputs': []},\n",
       "  {'function': 'same_shape', 'inputs': [4], 'value_inputs': []},\n",
       "  {'function': 'count', 'inputs': [5], 'value_inputs': []}],\n",
       " 'question': 'How many other things are there of the same shape as the tiny cyan matte object?',\n",
       " 'question_family_index': 43,\n",
       " 'question_index': 1,\n",
       " 'split': 'train'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_train[\"questions\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Finding maximum length of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:01<00:00, 570355.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Max question length\n",
    "max_question_length = 0\n",
    "\n",
    "for q in tqdm.tqdm(questions_train[\"questions\"]):\n",
    "    curr_question_length = len(q['question'].split())\n",
    "    if curr_question_length > max_question_length:\n",
    "        max_question_length = curr_question_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 491482.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check max question length in val\n",
    "val_max_question_length = 0\n",
    "for q in tqdm.tqdm(questions_val[\"questions\"]):\n",
    "    curr_question_length = len(q['question'].split())\n",
    "    if curr_question_length > val_max_question_length:\n",
    "        val_max_question_length = curr_question_length\n",
    "print(val_max_question_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149988/149988 [00:00<00:00, 554443.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check max question length in test\n",
    "test_max_question_length = 0\n",
    "for q in tqdm.tqdm(questions_test[\"questions\"]):\n",
    "    curr_question_length = len(q['question'].split())\n",
    "    if curr_question_length > test_max_question_length:\n",
    "        test_max_question_length = curr_question_length\n",
    "print(test_max_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Finding question vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove punctuation\n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:07<00:00, 88393.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Question corpus\n",
    "question_corpus = []\n",
    "\n",
    "for q in tqdm.tqdm(questions_train[\"questions\"]):\n",
    "    question_corpus += [w.translate(table) for w in q['question'].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12867378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question vocabulary\n",
    "question_vocabulary = sorted(set(question_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anything',\n",
       " 'are',\n",
       " 'as',\n",
       " 'ball',\n",
       " 'balls',\n",
       " 'behind',\n",
       " 'big',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'blue',\n",
       " 'both',\n",
       " 'brown',\n",
       " 'color',\n",
       " 'cube',\n",
       " 'cubes',\n",
       " 'cyan',\n",
       " 'cylinder',\n",
       " 'cylinders',\n",
       " 'do',\n",
       " 'does',\n",
       " 'either',\n",
       " 'else',\n",
       " 'equal',\n",
       " 'fewer',\n",
       " 'front',\n",
       " 'gray',\n",
       " 'greater',\n",
       " 'green',\n",
       " 'has',\n",
       " 'have',\n",
       " 'how',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'large',\n",
       " 'left',\n",
       " 'less',\n",
       " 'made',\n",
       " 'many',\n",
       " 'material',\n",
       " 'matte',\n",
       " 'metal',\n",
       " 'metallic',\n",
       " 'more',\n",
       " 'number',\n",
       " 'object',\n",
       " 'objects',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'other',\n",
       " 'purple',\n",
       " 'red',\n",
       " 'right',\n",
       " 'rubber',\n",
       " 'same',\n",
       " 'shape',\n",
       " 'shiny',\n",
       " 'side',\n",
       " 'size',\n",
       " 'small',\n",
       " 'sphere',\n",
       " 'spheres',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'tiny',\n",
       " 'to',\n",
       " 'visible',\n",
       " 'what',\n",
       " 'yellow']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:01<00:00, 88512.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabs are equal!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking question vocabulary with val\n",
    "equal_vocabs = True\n",
    "val_question_corpus = []\n",
    "for q in tqdm.tqdm(questions_val[\"questions\"]):\n",
    "    val_question_corpus += [w.translate(table) for w in q['question'].lower().split()]\n",
    "val_question_vocabulary = sorted(set(val_question_corpus))\n",
    "if len(val_question_vocabulary) != len(question_vocabulary):\n",
    "    print(\"Lengths not same! Vocabs are not equal!\")\n",
    "    equal_vocabs = False\n",
    "if equal_vocabs:\n",
    "    for i in range(len(val_question_vocabulary)):\n",
    "        if val_question_vocabulary[i] != question_vocabulary[i]:\n",
    "            print(\"Vocabs not equal at:\", i, question_vocabulary[i], val_question_vocabulary[i])\n",
    "            equal_vocabs = False\n",
    "            break\n",
    "if equal_vocabs:\n",
    "    print(\"Vocabs are equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149988/149988 [00:01<00:00, 88310.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabs are equal!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking question vocabulary with test (just in case of OOV)\n",
    "equal_vocabs = True\n",
    "test_question_corpus = []\n",
    "for q in tqdm.tqdm(questions_test[\"questions\"]):\n",
    "    test_question_corpus += [w.translate(table) for w in q['question'].lower().split()]\n",
    "test_question_vocabulary = sorted(set(test_question_corpus))\n",
    "if len(test_question_vocabulary) != len(question_vocabulary):\n",
    "    print(\"Lengths not same! Vocabs are not equal!\")\n",
    "    equal_vocabs = False\n",
    "if equal_vocabs:\n",
    "    for i in range(len(test_question_vocabulary)):\n",
    "        if test_question_vocabulary[i] != question_vocabulary[i]:\n",
    "            print(\"Vocabs not equal at:\", i, question_vocabulary[i], test_question_vocabulary[i])\n",
    "            equal_vocabs = False\n",
    "            break\n",
    "if equal_vocabs:\n",
    "    print(\"Vocabs are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Converting questions to lists of word indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make list of word indices in reverse order of words,\n",
    "as is usual practice for inputs to LSTMs, i.e.\n",
    "If question is \"what is this blue thing?\", the word indices being 1, 2, 3, 4, 5 respectively,\n",
    "and *max_q_len* is 7, then the word_indices list shall be [0, 0, 5, 4, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:26<00:00, 26436.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "questions_word_indices_train = np.zeros((len(questions_train[\"questions\"]), max_question_length))\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_train[\"questions\"])):\n",
    "    for iw, w in enumerate(q['question'].lower().split()):\n",
    "        questions_word_indices_train[iq][-iw-1] = question_vocabulary.index(w.translate(table)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:05<00:00, 26114.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Val\n",
    "questions_word_indices_val = np.zeros((len(questions_val[\"questions\"]), max_question_length))\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_val[\"questions\"])):\n",
    "    for iw, w in enumerate(q['question'].lower().split()):\n",
    "        questions_word_indices_val[iq][-iw-1] = question_vocabulary.index(w.translate(table)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149988/149988 [00:05<00:00, 26546.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "questions_word_indices_test = np.zeros((len(questions_test[\"questions\"]), max_question_length))\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_test[\"questions\"])):\n",
    "    for iw, w in enumerate(q['question'].lower().split()):\n",
    "        questions_word_indices_test[iq][-iw-1] = question_vocabulary.index(w.translate(table)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a yellow cube that is behind the large metal object; is its size the same as the block on the right side of the small gray shiny object?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  52.,  64.,  31.,  67.,  72.,  54.,  65.,  60.,  72.,\n",
       "        55.,  13.,  72.,   8.,  62.,  72.,  66.,  40.,  38.,  52.,  48.,\n",
       "        41.,  72.,  11.,  38.,  71.,  19.,  80.,   1.,  38.,  73.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_word_indices_test[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Image indices for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:00<00:00, 1252530.01it/s]\n"
     ]
    }
   ],
   "source": [
    "questions_image_indices_train = np.zeros((len(questions_train[\"questions\"])), dtype=int)\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_train[\"questions\"])):\n",
    "    questions_image_indices_train[iq] = q['image_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 1216673.60it/s]\n"
     ]
    }
   ],
   "source": [
    "questions_image_indices_val = np.zeros((len(questions_val[\"questions\"])), dtype=int)\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_val[\"questions\"])):\n",
    "    questions_image_indices_val[iq] = q['image_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149988/149988 [00:00<00:00, 1155775.00it/s]\n"
     ]
    }
   ],
   "source": [
    "questions_image_indices_test = np.zeros((len(questions_test[\"questions\"])), dtype=int)\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_test[\"questions\"])):\n",
    "    questions_image_indices_test[iq] = q['image_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Finding answer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:00<00:00, 1202112.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Answers corpus\n",
    "answers_corpus = []\n",
    "\n",
    "for q in tqdm.tqdm(questions_train[\"questions\"]):\n",
    "    answers_corpus.append(q['answer'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer vocabulary\n",
    "answers_vocabulary = sorted(set(answers_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '10',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'blue',\n",
       " 'brown',\n",
       " 'cube',\n",
       " 'cyan',\n",
       " 'cylinder',\n",
       " 'gray',\n",
       " 'green',\n",
       " 'large',\n",
       " 'metal',\n",
       " 'no',\n",
       " 'purple',\n",
       " 'red',\n",
       " 'rubber',\n",
       " 'small',\n",
       " 'sphere',\n",
       " 'yellow',\n",
       " 'yes']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 1097413.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Vocabs are equal!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check answer_vocabulary in val\n",
    "equal_vocabs = True\n",
    "val_answers_corpus = []\n",
    "for q in tqdm.tqdm(questions_val[\"questions\"]):\n",
    "    val_answers_corpus.append(q['answer'].lower())\n",
    "# Vocab\n",
    "val_answers_vocabulary = sorted(set(val_answers_corpus))\n",
    "print(len(val_answers_vocabulary))\n",
    "if len(val_answers_vocabulary) != len(answers_vocabulary):\n",
    "    print(\"Lengths not same! Vocabs are not equal!\")\n",
    "    equal_vocabs = False\n",
    "if equal_vocabs:\n",
    "    for i in range(len(val_answers_vocabulary)):\n",
    "        if val_answers_vocabulary[i] != answers_vocabulary[i]:\n",
    "            print(\"Vocabs not equal at:\", i, answers_vocabulary[i], val_answers_vocabulary[i])\n",
    "            equal_vocabs = False\n",
    "            break\n",
    "if equal_vocabs:\n",
    "    print(\"Vocabs are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Finding answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:00<00:00, 1249439.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "answers_train = []\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_train[\"questions\"])):\n",
    "    answers_train.append(q['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 1179115.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Val\n",
    "answers_val = []\n",
    "for iq, q in enumerate(tqdm.tqdm(questions_val[\"questions\"])):\n",
    "    answers_val.append(q['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Converting answers to one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:00<00:00, 776030.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "one_hot_answers_train = np.zeros((len(answers_train), len(answers_vocabulary)))\n",
    "for i, answer in enumerate(tqdm.tqdm(answers_train)):\n",
    "    one_hot_answers_train[i][answers_vocabulary.index(answer)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 730350.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Val\n",
    "one_hot_answers_val = np.zeros((len(answers_val), len(answers_vocabulary)))\n",
    "for i, answer in enumerate(tqdm.tqdm(answers_val)):\n",
    "    one_hot_answers_val[i][answers_vocabulary.index(answer)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Saving/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.savez(\"question_answer_stuff\",\n",
    "         max_question_length=max_question_length,\n",
    "         questions_image_indices_train=questions_image_indices_train,\n",
    "         questions_word_indices_train=questions_word_indices_train,\n",
    "         questions_image_indices_val=questions_image_indices_val,\n",
    "         questions_word_indices_val=questions_word_indices_val,\n",
    "         questions_image_indices_test=questions_image_indices_test,\n",
    "         questions_word_indices_test=questions_word_indices_test,\n",
    "         question_vocabulary=question_vocabulary,\n",
    "         answers_vocabulary=answers_vocabulary,\n",
    "         answers_train=answers_train,\n",
    "         answers_val=answers_val,\n",
    "         one_hot_answers_train=one_hot_answers_train,\n",
    "         one_hot_answers_val=one_hot_answers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "question_answer_stuff = np.load(\"question_answer_stuff.npz\")\n",
    "max_question_length = question_answer_stuff[\"max_question_length\"]\n",
    "questions_image_indices_train = question_answer_stuff[\"questions_image_indices_train\"]\n",
    "questions_word_indices_train = question_answer_stuff[\"questions_word_indices_train\"]\n",
    "questions_image_indices_val = question_answer_stuff[\"questions_image_indices_val\"]\n",
    "questions_word_indices_val = question_answer_stuff[\"questions_word_indices_val\"]\n",
    "questions_image_indices_test = question_answer_stuff[\"questions_image_indices_test\"]\n",
    "questions_word_indices_test = question_answer_stuff[\"questions_word_indices_test\"]\n",
    "answers_vocabulary = question_answer_stuff[\"answers_vocabulary\"]\n",
    "one_hot_answers_train = question_answer_stuff[\"one_hot_answers_train\"]\n",
    "one_hot_answers_val = question_answer_stuff[\"one_hot_answers_val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dealing with state descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Reading the json file containing descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .json files\n",
    "state_description_train = json.load(open(state_description_train_file))\n",
    "state_description_val = json.load(open(state_description_val_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directions': {'above': [0.0, 0.0, 1.0],\n",
       "  'behind': [-0.754490315914154, 0.6563112735748291, 0.0],\n",
       "  'below': [-0.0, -0.0, -1.0],\n",
       "  'front': [0.754490315914154, -0.6563112735748291, -0.0],\n",
       "  'left': [-0.6563112735748291, -0.7544902563095093, 0.0],\n",
       "  'right': [0.6563112735748291, 0.7544902563095093, -0.0]},\n",
       " 'image_filename': 'CLEVR_train_000000.png',\n",
       " 'image_index': 0,\n",
       " 'objects': [{'3d_coords': [-1.3705521821975708,\n",
       "    2.0794010162353516,\n",
       "    0.699999988079071],\n",
       "   'color': 'blue',\n",
       "   'material': 'rubber',\n",
       "   'pixel_coords': [269, 88, 12.661545753479004],\n",
       "   'rotation': 269.8517172617167,\n",
       "   'shape': 'cube',\n",
       "   'size': 'large'},\n",
       "  {'3d_coords': [-2.9289753437042236, -1.7488206624984741, 0.699999988079071],\n",
       "   'color': 'green',\n",
       "   'material': 'metal',\n",
       "   'pixel_coords': [93, 108, 11.522202491760254],\n",
       "   'rotation': 292.2219458666971,\n",
       "   'shape': 'cylinder',\n",
       "   'size': 'large'},\n",
       "  {'3d_coords': [1.5515961647033691, 0.6776641607284546, 0.3499999940395355],\n",
       "   'color': 'cyan',\n",
       "   'material': 'rubber',\n",
       "   'pixel_coords': [319, 162, 10.045343399047852],\n",
       "   'rotation': 25.545135239473026,\n",
       "   'shape': 'cube',\n",
       "   'size': 'small'},\n",
       "  {'3d_coords': [-0.25301405787467957, -2.3089325428009033, 0.699999988079071],\n",
       "   'color': 'brown',\n",
       "   'material': 'metal',\n",
       "   'pixel_coords': [132, 159, 9.392304420471191],\n",
       "   'rotation': 327.3489188814305,\n",
       "   'shape': 'cylinder',\n",
       "   'size': 'large'},\n",
       "  {'3d_coords': [1.018894076347351, -1.93693208694458, 0.3499999940395355],\n",
       "   'color': 'gray',\n",
       "   'material': 'rubber',\n",
       "   'pixel_coords': [192, 197, 8.907766342163086],\n",
       "   'rotation': 6.325183772442613,\n",
       "   'shape': 'cube',\n",
       "   'size': 'small'},\n",
       "  {'3d_coords': [0.43993687629699707, 2.9987525939941406, 0.699999988079071],\n",
       "   'color': 'brown',\n",
       "   'material': 'metal',\n",
       "   'pixel_coords': [353, 100, 11.964213371276855],\n",
       "   'rotation': 25.96049348342493,\n",
       "   'shape': 'sphere',\n",
       "   'size': 'large'}],\n",
       " 'relationships': {'behind': [[],\n",
       "   [0, 5],\n",
       "   [0, 1, 5],\n",
       "   [0, 1, 2, 5],\n",
       "   [0, 1, 2, 3, 5],\n",
       "   [0]],\n",
       "  'front': [[1, 2, 3, 4, 5], [2, 3, 4], [3, 4], [4], [], [1, 2, 3, 4]],\n",
       "  'left': [[1, 3, 4], [], [0, 1, 3, 4], [1], [1, 3], [0, 1, 2, 3, 4]],\n",
       "  'right': [[2, 5], [0, 2, 3, 4, 5], [5], [0, 2, 4, 5], [0, 2, 5], []]},\n",
       " 'split': 'train'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_description_train[\"scenes\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Count the maximum number of objects in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:00<00:00, 968115.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count the max number of objects in an image\n",
    "max_number_of_objects_in_scene = 0\n",
    "\n",
    "for i in tqdm.tqdm(range(len(state_description_train['scenes']))):\n",
    "    curr_number_of_objects = len(state_description_train['scenes'][i]['objects'])\n",
    "    if curr_number_of_objects > max_number_of_objects_in_scene:\n",
    "        max_number_of_objects_in_scene = curr_number_of_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_number_of_objects_in_scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Make the *state_description_matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def my_color(color_text):\n",
    "    if color_text == 'gray':\n",
    "        return [.5, .5, .5]\n",
    "    elif color_text == 'blue':\n",
    "        return [0., 0., 1.]\n",
    "    elif color_text == 'brown':\n",
    "        return [165/255., 42/255., 42/255.]\n",
    "    elif color_text == 'yellow':\n",
    "        return [1., 1., 0.]\n",
    "    elif color_text == 'red':\n",
    "        return [1., 0., 0.]\n",
    "    elif color_text == 'green':\n",
    "        return [0., 1., 0.]\n",
    "    elif color_text == 'purple':\n",
    "        return [.5, 0., .5]\n",
    "    elif color_text == 'cyan':\n",
    "        return [0., 1., 1.]\n",
    "    \n",
    "def my_shape_index(shape):\n",
    "    if shape == 'cube':\n",
    "        return 0\n",
    "    elif shape == 'sphere':\n",
    "        return 1\n",
    "    elif shape == 'cylinder':\n",
    "        return 2\n",
    "    \n",
    "def my_material_index(material):\n",
    "    if material == 'rubber':\n",
    "        return 0\n",
    "    elif material == 'metal':\n",
    "        return 1\n",
    "    \n",
    "def my_size_index(size):\n",
    "    if size == 'small':\n",
    "        return 0\n",
    "    elif size == 'large':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state_description_matrix(state_description, max_number_of_objects_in_scene=10):\n",
    "    \n",
    "    # Define state description matrix\n",
    "    # features – 3D coordinates (x, y, z);\n",
    "    #            color (r, g, b) {\"gray\", \"blue\", \"brown\", \"yellow\", \"red\", \"green\", \"purple\", or \"cyan\"};\n",
    "    #            shape (\"cube\", \"sphere\", or \"cylinder\");\n",
    "    #            material (\"rubber\" or \"metal\");\n",
    "    #            size (\"small\" or \"large\")\n",
    "    # state_description_matrix === (n, max_number_of_objects_in_scene, objects_features_dim)\n",
    "    state_description_matrix = np.zeros((len(state_description['scenes']), max_number_of_objects_in_scene, 13))\n",
    "\n",
    "    # Make state description matrix\n",
    "    for i in tqdm.tqdm(range(len(state_description['scenes']))):\n",
    "        for o in range(len(state_description['scenes'][i]['objects'])):\n",
    "            state_description_matrix[i][o][0:3] = state_description['scenes'][i]['objects'][o]['3d_coords']\n",
    "            state_description_matrix[i][o][3:6] = my_color(state_description['scenes'][i]['objects'][o]['color'])\n",
    "            state_description_matrix[i][o][6 + my_shape_index(state_description['scenes'][i]['objects'][o]['shape'])] = 1\n",
    "            state_description_matrix[i][o][9 + my_material_index(state_description['scenes'][i]['objects'][o]['material'])] = 1\n",
    "            state_description_matrix[i][o][11 + my_size_index(state_description['scenes'][i]['objects'][o]['size'])] = 1\n",
    "    \n",
    "    return state_description_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:02<00:00, 24569.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "state_description_matrix_train = make_state_description_matrix(state_description_train, max_number_of_objects_in_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of dimensions in object\n",
    "object_features_dim = state_description_matrix_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 24408.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Val\n",
    "state_description_matrix_val = make_state_description_matrix(state_description_val, max_number_of_objects_in_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 10, 13)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_description_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.37055218,  2.07940102,  0.69999999,  0.        ,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ],\n",
       "       [-2.92897534, -1.74882066,  0.69999999,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 1.55159616,  0.67766416,  0.34999999,  0.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  0.        ],\n",
       "       [-0.25301406, -2.30893254,  0.69999999,  0.64705882,  0.16470588,\n",
       "         0.16470588,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 1.01889408, -1.93693209,  0.34999999,  0.5       ,  0.5       ,\n",
       "         0.5       ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  0.        ],\n",
       "       [ 0.43993688,  2.99875259,  0.69999999,  0.64705882,  0.16470588,\n",
       "         0.16470588,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_description_matrix_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  -2.99998259544 to 2.99999952316\n",
      "y:  -2.99999976158 to 2.99997830391\n",
      "z:  0.0 to 0.699999988079\n"
     ]
    }
   ],
   "source": [
    "# Range of 3d coords\n",
    "x_min = np.min(state_description_matrix_train[:, :, 0])\n",
    "x_max = np.max(state_description_matrix_train[:, :, 0])\n",
    "y_min = np.min(state_description_matrix_train[:, :, 1])\n",
    "y_max = np.max(state_description_matrix_train[:, :, 1])\n",
    "z_min = np.min(state_description_matrix_train[:, :, 2])\n",
    "z_max = np.max(state_description_matrix_train[:, :, 2])\n",
    "print(\"x: \", x_min, \"to\", x_max)\n",
    "print(\"y: \", y_min, \"to\", y_max)\n",
    "print(\"z: \", z_min, \"to\", z_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing 3d coords in train (in cases where objects are present)\n",
    "state_description_matrix_train[:, :, 0] = (state_description_matrix_train[:, :, 0] - x_min)/(x_max - x_min)*(state_description_matrix_train[:, :, 0] != 0)\n",
    "state_description_matrix_train[:, :, 1] = (state_description_matrix_train[:, :, 1] - y_min)/(y_max - y_min)*(state_description_matrix_train[:, :, 1] != 0)\n",
    "state_description_matrix_train[:, :, 2] = (state_description_matrix_train[:, :, 2] - z_min)/(z_max - z_min)*(state_description_matrix_train[:, :, 2] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing 3d coords in val (in cases where objects are present)\n",
    "state_description_matrix_val[:, :, 0] = (state_description_matrix_val[:, :, 0] - x_min)/(x_max - x_min)*(state_description_matrix_val[:, :, 0] != 0)\n",
    "state_description_matrix_val[:, :, 1] = (state_description_matrix_val[:, :, 1] - y_min)/(y_max - y_min)*(state_description_matrix_val[:, :, 1] != 0)\n",
    "state_description_matrix_val[:, :, 2] = (state_description_matrix_val[:, :, 2] - z_min)/(z_max - z_min)*(state_description_matrix_val[:, :, 2] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27157254,  0.84656989,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ],\n",
       "       [ 0.01183458,  0.20853061,  1.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 0.75859872,  0.61294623,  0.5       ,  0.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  0.        ],\n",
       "       [ 0.45782945,  0.11517829,  1.        ,  0.64705882,  0.16470588,\n",
       "         0.16470588,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 0.66981477,  0.17717859,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  0.        ],\n",
       "       [ 0.57332162,  0.99979571,  1.        ,  0.64705882,  0.16470588,\n",
       "         0.16470588,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_description_matrix_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Saving/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.savez(\"state_description_matrix\",\n",
    "         max_number_of_objects_in_scene=max_number_of_objects_in_scene,\n",
    "         object_features_dim=object_features_dim,\n",
    "         state_description_matrix_train=state_description_matrix_train,\n",
    "         state_description_matrix_val=state_description_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "state_description_matrix = np.load(\"state_description_matrix.npz\")\n",
    "max_number_of_objects_in_scene = state_description_matrix[\"max_number_of_objects_in_scene\"]\n",
    "object_features_dim = state_description_matrix[\"object_features_dim\"]\n",
    "state_description_matrix_train = state_description_matrix[\"state_description_matrix_train\"]\n",
    "state_description_matrix_val = state_description_matrix[\"state_description_matrix_val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Make training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training sample given to the Relation Network is a (scene, question) pair, the scene being image or state_description.\n",
    "\n",
    "The *scene* or *state_description_matrix* is an *(S x max_number_of_objects_in_scene x object_features_dim)* array, while the questions matrix is a *(Q x max_question_length)* array.\n",
    "\n",
    "The training batch needs to be *[scenes_input, questions_input]*, where *scenes_input* is an *(n x max_number_of_objects_in_scene x object_features_dim)* array, and *questions_input* is an *(n x max_question_length)* array. Each row consists of a *question* and the *scene* (state_description) of the image corresponding to the question.\n",
    "\n",
    "**rn_input** = [**scenes_input**, **questions_input**], **rn_output** = [**one_hot_answers**]\n",
    "\n",
    "**scenes_input** === *(n, max_number_of_objects_in_scene, object_features_dim)*\n",
    "\n",
    "**questions_input** === *(n, max_question_length)*\n",
    "\n",
    "**one_hot_answers** === *(n, answer_vocabulary_length)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699989/699989 [00:01<00:00, 613537.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train samples\n",
    "# rn_input = [scenes_input_train, questions_input_train]\n",
    "# scenes_input === n x max_number_of_objects_in_scene x object_features_dim\n",
    "# questions_input === n x max_question_length\n",
    "questions_input_train = questions_word_indices_train\n",
    "scenes_input_train = np.zeros((len(questions_input_train),\n",
    "                               state_description_matrix_train.shape[1],\n",
    "                               state_description_matrix_train.shape[2]))\n",
    "for i, image_index in enumerate(tqdm.tqdm(questions_image_indices_train)):\n",
    "    scenes_input_train[i] = state_description_matrix_train[image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149991/149991 [00:00<00:00, 535444.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Val samples\n",
    "questions_input_val = questions_word_indices_val\n",
    "scenes_input_val = np.zeros((len(questions_input_val),\n",
    "                               state_description_matrix_val.shape[1],\n",
    "                               state_description_matrix_val.shape[2]))\n",
    "for i, image_index in enumerate(tqdm.tqdm(questions_image_indices_val)):\n",
    "    scenes_input_val[i] = state_description_matrix_val[image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples\n",
    "questions_input_test = questions_word_indices_test\n",
    "# scenes_input_test = np.zeros((len(questions_input_test),\n",
    "#                                state_description_matrix_test.shape[1],\n",
    "#                                state_description_matrix_test.shape[2]))\n",
    "# for i, image_index in enumerate(tqdm.tqdm(questions_image_indices_test)):\n",
    "#     scenes_input_test[i] = state_description_matrix_test[image_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Example val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example validation batch of 50 samples\n",
    "example_val_idx = np.random.choice(len(questions_input_val), 50)\n",
    "questions_input_val_example_batch = questions_input_val[example_val_idx]\n",
    "scenes_input_val_example_batch = scenes_input_val[example_val_idx]\n",
    "one_hot_answers_val_example_batch = one_hot_answers_val[example_val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Making the Relation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first look at individual functions to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_network(max_number_of_objects_in_scene=10, object_features_dim=13, max_question_length=43,\n",
    "                     WORD_EMBEDDING_DIM=32, LSTM_UNITS=256,\n",
    "                     G_FC1=512, G_FC2=512, G_FC3=512, G_FC4=512,\n",
    "                     F_FC1=512, F_FC2=1024, F_DROPOUT2=0.02, answers_vocabulary_length=29):\n",
    "    \n",
    "    # scenes_input === (n, max_number_of_objects_in_scene, object_features_dim)\n",
    "    # questions_input === (n, max_question_length)\n",
    "    \n",
    "    # Inputs\n",
    "    scenes_input_tensor = Input(shape=(max_number_of_objects_in_scene, object_features_dim,))\n",
    "    questions_input_tensor = Input(shape=(max_question_length,))\n",
    "    \n",
    "    # Process input for g_theta from [scenes_input, questions_input],\n",
    "    # by making all object pairs for each scene and concatenating question in each pair\n",
    "    # g_input === (n,\n",
    "    #              max_number_of_objects_in_scene*max_number_of_objects_in_scene,\n",
    "    #              2*object_features_dim+question_features_dim)\n",
    "    g_input = process_scenes_and_questions(max_number_of_objects_in_scene=max_number_of_objects_in_scene,\n",
    "                                           object_features_dim=object_features_dim,\n",
    "                                           max_question_length=max_question_length,\n",
    "                                           WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM,\n",
    "                                           LSTM_UNITS=LSTM_UNITS)([scenes_input_tensor, questions_input_tensor])\n",
    "    \n",
    "    # G\n",
    "    # Run each g_input through g_model to make g_output\n",
    "    # g_model runs g_theta on each object_pair+question in each sample\n",
    "    # g_output === (n, G_FC4)\n",
    "    g_output = g_model(max_number_of_objects_in_scene=max_number_of_objects_in_scene,\n",
    "                       object_features_dim=object_features_dim,\n",
    "                       question_features_dim=LSTM_UNITS,\n",
    "                       G_FC1=G_FC1, G_FC2=G_FC2, G_FC3=G_FC3, G_FC4=G_FC4)(g_input)\n",
    "    \n",
    "    # F\n",
    "    # f_output === (n, answer_vocabulary_length)\n",
    "    f_output = f_phi(input_dim=G_FC4,\n",
    "                     F_FC1=F_FC1, F_FC2=F_FC2, F_DROPOUT2=F_DROPOUT2, F_FC3=answers_vocabulary_length)(g_output)\n",
    "    \n",
    "    relation_network = Model(inputs=[scenes_input_tensor, questions_input_tensor], outputs=[f_output])\n",
    "    \n",
    "    return relation_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, I shall create the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 *process_scenes_and_questions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Convert the word_indices in each question to an embedding and pass through an LSTM to make question_features\n",
    "\n",
    "2) Make all object pairs in each scene and concatenate with question_features\n",
    "\n",
    "Every object_pair+question_features is the input to g_theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenes_and_questions(max_number_of_objects_in_scene=10,\n",
    "                                 object_features_dim=13,\n",
    "                                 max_question_length=43,\n",
    "                                 WORD_EMBEDDING_DIM=32,\n",
    "                                 LSTM_UNITS=256):\n",
    "    '''\n",
    "    scenes_input === n x max_number_of_objects_in_scene x object_features_dim\n",
    "    '''\n",
    "    # Inputs to model\n",
    "    scenes_input = Input(shape=(max_number_of_objects_in_scene, object_features_dim,))\n",
    "    questions_input = Input(shape=(max_question_length,))\n",
    "\n",
    "    # Make question_features using Embedding+LSTM\n",
    "    question_embeddings = Embedding(max_question_length, WORD_EMBEDDING_DIM, mask_zero=True)(questions_input)\n",
    "    question_features = LSTM(LSTM_UNITS)(question_embeddings)\n",
    "\n",
    "    # Make all object pairs and concatenate question_features to each pair \n",
    "    g_input = MakeGInput()([scenes_input, question_features])\n",
    "    \n",
    "    return Model(inputs=[scenes_input, questions_input], outputs=[g_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MakeGInput requires a lot of tensor operations, I made it a Keras Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeGInput(Layer):\n",
    "    def __init__(self):\n",
    "        super(MakeGInput, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(MakeGInput, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        '''\n",
    "        inputs[0] = scenes_input === (n, max_number_of_objects_in_scene, object_features_dim)\n",
    "        inputs[1] = question_features === (n, questions_LSTM_dim)\n",
    "        '''\n",
    "        scenes_input = inputs[0]\n",
    "        question_features = inputs[1]\n",
    "        \n",
    "        # Calc\n",
    "        max_number_of_objects_in_scene = scenes_input.shape[1]\n",
    "        \n",
    "        # MAKE ALL OBJECT PAIRS, AND CONCATENATE QUESTION\n",
    "        \n",
    "        # Arrange one of the pair\n",
    "        scenes_input_i = K.expand_dims(scenes_input, axis=1)\n",
    "        \n",
    "        # Repeat this pair in axis 1\n",
    "        scenes_input_i1 = K.repeat_elements(scenes_input_i, rep=max_number_of_objects_in_scene, axis=1)\n",
    "\n",
    "        # Reshape the second of the pair\n",
    "        scenes_input_j = K.expand_dims(scenes_input, axis=2)\n",
    "\n",
    "        # Arrange question_features to concatenate\n",
    "        question_features1 = K.expand_dims(question_features, axis=1)\n",
    "        question_features2 = K.repeat_elements(question_features1, rep=max_number_of_objects_in_scene, axis=1)\n",
    "        question_features3 = K.expand_dims(question_features2, axis=2)\n",
    "\n",
    "        # Concatenate question_features to second pair\n",
    "        scenes_input_j_and_q = K.concatenate([scenes_input_j, question_features3], axis=-1)\n",
    "\n",
    "        # Repeat second of the pair i axis 2\n",
    "        scenes_input_j_and_q1 = K.repeat_elements(scenes_input_j_and_q, rep=max_number_of_objects_in_scene, axis=2)\n",
    "\n",
    "        # Concatenate all\n",
    "        g_input0 = K.concatenate([scenes_input_i1, scenes_input_j_and_q1], axis=-1)\n",
    "        \n",
    "        # Reshape to have all object pairs, i.e. 10*10=100 elements per scene-question sample\n",
    "        g_input = K.reshape(g_input0,\n",
    "                            (-1,\n",
    "                             max_number_of_objects_in_scene*max_number_of_objects_in_scene,\n",
    "                             g_input0.shape[-1].value))\n",
    "\n",
    "        return g_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 *g_model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*g_model* runs *g_theta* on each object_pair+question_feature in each scene+question sample, and sums up the g_theta_outputs of each scene+question sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_model(max_number_of_objects_in_scene=10, object_features_dim=13, question_features_dim=256,\n",
    "            G_FC1=512, G_FC2=512, G_FC3=512, G_FC4=512):\n",
    "    \n",
    "    number_of_object_pairs = max_number_of_objects_in_scene * max_number_of_objects_in_scene\n",
    "    features_dim = 2 * object_features_dim+question_features_dim\n",
    "    \n",
    "    g_input = Input(shape=(number_of_object_pairs, features_dim))\n",
    "    \n",
    "    # Convert g_input === (n, number_of_object_pairs, features_dim) into\n",
    "    # g_theta_input === (n*number_of_object_pairs, features_dim) so that\n",
    "    # g_theta can be applied on every object pair in every question+scene sample\n",
    "    g_theta_input = Lambda(condense_batch_size_and_number_of_object_pairs)(g_input)\n",
    "    \n",
    "    # Apply g_theta\n",
    "    g_theta_output = g_theta(features_dim=features_dim,\n",
    "                             G_FC1=G_FC1, G_FC2=G_FC2, G_FC3=G_FC3, G_FC4=G_FC4)(g_theta_input)\n",
    "    \n",
    "    # Convert g_theta_output === (n*number_of_object_pairs, G_FC4) into\n",
    "    # g_theta_outputs === (n, number_of_object_pairs, G_FC4) so that the\n",
    "    # the g_theta outputs of all object pairs (in each question+scene sample) can be summed\n",
    "    g_theta_outputs = Lambda(expand_batch_size_and_number_of_object_pairs,\n",
    "                             arguments={'number_of_object_pairs': number_of_object_pairs})(g_theta_output)\n",
    "    \n",
    "    # Sum the g_theta outputs of all object pairs (in each question+scene sample)\n",
    "    g_theta_outputs_sum = Lambda(sum_g_theta_outputs)(g_theta_outputs)\n",
    "    \n",
    "    return Model(inputs=[g_input], outputs=[g_theta_outputs_sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g_theta has 4 Dense layers with relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_theta(features_dim=282, G_FC1=512, G_FC2=512, G_FC3=512, G_FC4=512):\n",
    "    g_theta_input = Input(shape=(features_dim,))\n",
    "    x = Dense(G_FC1, activation='relu')(g_theta_input)\n",
    "    x = Dense(G_FC2, activation='relu')(x)\n",
    "    x = Dense(G_FC3, activation='relu')(x)\n",
    "    g_theta_output = Dense(G_FC4, activation='relu')(x)\n",
    "    return Model(inputs=[g_theta_input], outputs=[g_theta_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_batch_size_and_number_of_object_pairs(x):\n",
    "    return K.reshape(x, (-1, x.shape[-1].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_batch_size_and_number_of_object_pairs(x, number_of_object_pairs=100):\n",
    "    return K.reshape(x, (-1, number_of_object_pairs, x.shape[-1].value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum_g_output sums the g_theta_outputs of each scene+question sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_g_theta_outputs(g_theta_outputs):\n",
    "    return K.sum(g_theta_outputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 *f_phi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_phi(input_dim=512, F_FC1=512, F_FC2=1024, F_DROPOUT2=0.02, F_FC3=29):\n",
    "    f_input = Input(shape=(input_dim,))\n",
    "    x = Dense(F_FC1, activation='relu')(f_input)\n",
    "    x = Dense(F_FC2, activation='relu')(x)\n",
    "    x = Dropout(F_DROPOUT2)(x)\n",
    "    f_output = Dense(F_FC3, activation='softmax')(x)\n",
    "    return Model(inputs=[f_input], outputs=[f_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Checking relation_network using example val batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 29)\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "rn = relation_network()\n",
    "b = rn.predict([scenes_input_val_example_batch, questions_input_val_example_batch])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the relation network\n",
    "rn = relation_network(max_number_of_objects_in_scene=max_number_of_objects_in_scene,\n",
    "                      object_features_dim=object_features_dim,\n",
    "                      max_question_length=max_question_length,\n",
    "                      answers_vocabulary_length=len(answers_vocabulary),\n",
    "                      WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, LSTM_UNITS=LSTM_UNITS,\n",
    "                      G_FC1=G_FC1, G_FC2=G_FC2, G_FC3=G_FC3, G_FC4=G_FC4,\n",
    "                      F_FC1=F_FC1, F_FC2=F_FC2, F_DROPOUT2=F_DROPOUT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "rn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "rn.fit(x=[scenes_input_train, questions_input_train], y=[one_hot_answers_train],\n",
    "       batch_size=batch_size, epochs=epochs, callbacks=None,\n",
    "       validation_data=([scenes_input_val, questions_input_val], one_hot_answers_val),\n",
    "       shuffle=True, class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_preds_test = rn.predict([scenes_input_train, questions_input_test], batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions into txt file\n",
    "with open(\"rn_pred_test.txt\", 'w') as f:\n",
    "    for softmax_pred in rn_preds_test:\n",
    "        f.write(answers_vocabulary[np.argmax(softmax_pred)] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
